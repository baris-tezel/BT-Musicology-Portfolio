---
title: "Baris Tezel Computational Musicology"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
---


### Preliminary Data Visualizations for 3 Afro-Francophonic Artists: Aya Nakamura, Stromae, and Maitre Gims. 


```{r}
library(tidyverse)
library(spotifyr)
library(compmus)

AYA <- get_playlist_audio_features("", "37i9dQZF1DZ06evO4zlQhH?si=2556e1ed8179453b")
GIMS <- get_playlist_audio_features("", "37i9dQZF1DZ06evO0lhGr6?si=41c043a364be43d5")
STROMAE <- get_playlist_audio_features("", "37i9dQZF1DX3NT4PRVyEpr?si=c639b1dce2344d94")

artists <-
  bind_rows(
    AYA %>% mutate(category = "Aya Nakamura"),
    GIMS %>% mutate(category = "Maitre Gims"),
    STROMAE %>% mutate(category = "Stromae")
  )
artists %>%
  ggplot(aes(x = danceability)) +
  geom_histogram(binwidth = 0.1) +
  facet_wrap(~category)

artists %>%                    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = danceability,
      y = tempo,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(size = 0.1) +  # Add 'fringes' to show data distribution
  facet_wrap(~category)
```

***
My graph for Homework Assignment Week 7: I am trying to study the correlation between tempo and danceability primarily. However, I am also looking at the impacts that loudness and mode has on danceability and tempo. It seems that lower tempos, result in a higher danceability rating. On average, Stromae's music seems to be the less loud. His music also seems the less danceable but does have a higher tempo rating (on average) than the other two artists. Aya Nakamura seems to have the lowest Tempo rating, there might be a correlation between rappers and lower tempo ratings. Since Maitre Gims is a mix of rapper and vocalist, it seems correct that on average, his music seems to score in between Stromae and Aya in terms of tempo and danceability. Each section will also have an embedded Spotify link so that people can listen to the songs as they read through my analysis. So that the graphs make more sense. 


### Tempogram for Orphelin by Aya Nakamura

```{r}
Orphelin <- 
  get_tidy_audio_analysis("7ypQ1PtbClW0Aqe1ewS7h7")
Orphelin %>%
 tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
Orphelin %>%
tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
  

```

***

This is the tempogram. One of the final assignments. I decided to analyze both temprograms that I could have created as I was a bit unsure of which would be better to analyze. I would love to have some feed back on which tempogram I should use. (The first or second). Overall, Orphelin by Aya Nakamura seemed to have the highest BPM out of any of the songs in my corpus. It scored a BPM of 199. Overall one of the Tempograms cannot reach past 160 BPM, which is not able to capture the BPM of the song. The tempogram on the left includes 199 BPM but it does not seem as green there. In fact, it is greener at 400 BPM which is almost double the BPM of Orphelin. When I was personally conducting a TAP BPM, I also received a BPM of 190. The song is a mixture of electronic and afro-beats. This could be the result of the higher BPM.  

### Track-Level Summaries for Aya Nakamura and Stromae


```{r}
library(tidyverse)
library(spotifyr)
library(compmus)
Aya_Naka <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "37i9dQZF1DZ06evO4zlQhH?si=2556e1ed8179453b"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()
Stromae_Belgian <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "37i9dQZF1DX3NT4PRVyEpr?si=c639b1dce2344d94"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()
Afro_Francophonic <-
  Aya_Naka %>%
  mutate(artist = "Aya") %>%
  bind_rows(Stromae_Belgian %>% mutate(artist = "Stromae"))



Afro_Francophonic %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = artist,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Artist",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )
```

***
Through this graph, I was able to see that Stromae, on average, had a higher average bmp for his songs in comparison to Aya. However, it seems that Aya's music had relatively similar Mean tempos. I wonder if this is in correlation to the fact that she is a rapper, I wonder if rapping is more conducive to similar levels of Mean Tempo. The graph also shows that Aya's music has relatively similar levels of Standard Deviation in Tempo. Again, I wonder if this in in correlation to the types of music that she produces in comparison to Stromae's music. I am going to create a point which represents the mean tempo of both Artists so there is clearer difference. 

### Chodograms for the Song Ta Fete by Stromae

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

tf <-
  get_tidy_audio_analysis("7fWWbkok99Tvh8aly2Buox?si=cda517b36c944da1") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  )

tf %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "cosine",  # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***
This chordogram is for the song Ta Fete by Stromae for the first 175 seconds. I am still in the process of really learning how to analyze these types of graphs. However, from my preliminary analysis it seems that Bb;major and Eb:major are the most prevalent chords in this song. B:major, E:major, and D:major is also quite prevalent in this song although not as much as the aforementioned two. At around the 100 seconds mark almost all of the chords except C#:minor and Db:major are incredibly prevalent for about 20 seconds. This could be because at this specific point of the music ____

### Self-Selection Matrixes for Sapes Commes Jamais; Maitre Gims

```{r}
library(tidyverse)
library(spotifyr)
library(compmus)

sapesjamais <-
  get_tidy_audio_analysis("0CJ31BEjjl1tPIj0CKi9kH?si=1d722c329b994433") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  sapesjamais %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma"),
  sapesjamais %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

***
This is of the song, Sapes Comme Jamais, by Maitre Gims. I need help in interpreting graphs like this. The professor has done a great job so far but this specific type of graph is the hardest for me to interpret. It seems as if the Chroma and Timbre graphs light up the most in a patchwork type pattern. Specifically it is the greenest on the 50 and 110 second marks. I am still reviewing the chapters which further explain how to interpret these graphs, as I am still struggling with this concept. I found that the more I listen to his songs, the better I am at picking up what this graph is trying to explaion. 

### Chromograms for Djaja, Sapes Commes Jamais, and Ta Fete.


```{r}
djaja <-
  get_tidy_audio_analysis("7sKDftgGirHWTVFMtJoDoh?si=7cfc7d2b067949da") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

djaja %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

sapes <-
  get_tidy_audio_analysis("0CJ31BEjjl1tPIj0CKi9kH?si=1d722c329b994433") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

sapes %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

Taf <-
  get_tidy_audio_analysis("7fWWbkok99Tvh8aly2Buox?si=cda517b36c944da1") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

Taf %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***
These are of the songs: Djaja by Aya, Sapes Comme Jamais by Gims, and Ta Fete by Stromae. I will label the graphs accordingly soon. Djaja has the highest magnitude at G, F, and C. For Sapes Commes Jamais has the highest prevalence in F#|Gb,C#|Db and B. Stromae's song Ta Fete has the highest variance. However, there was a huge concentration on C#|Db and B. 

### Ceptogram for Djaja, Sapes Commes Jamais, and Ta Fete. 

```{r}
djaj <-
  get_tidy_audio_analysis("7sKDftgGirHWTVFMtJoDoh?si=7cfc7d2b067949da") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

djaj %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

sapescomme <-
  get_tidy_audio_analysis("0CJ31BEjjl1tPIj0CKi9kH?si=1d722c329b994433") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

sapescomme %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

tafete <-
  get_tidy_audio_analysis("7fWWbkok99Tvh8aly2Buox?si=cda517b36c944da1") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

tafete %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

```  

***
These are of the songs: Djaja by Aya, Sapes Comme Jamais by Gims, and Ta Fete by Stromae. I will label the graphs accordingly soon. In Djaja, it is concentrated mainly around c05 and c02. In fact c01-c06 is the most prominent for Aya's Djaja. Sapes Commes Jamais is definitely more spread out in it's concentration. However c02, has the highest magnitude through most of the 200 seconds. Stromae's Ta Fete also has a high magnitude in c02. However, c06 is also incredibly prevalent throughout this second. Overall, all three of these artists have the highest prevalanece throughout c01-04. I wonder if this is due to the link of them all being Afro-Francophonic artists. That commonality could be what links them together and creates a ceptogram that has a similar concentration. 
  
### Introduction to my Corpus; The Three Afro-Francophonic Artists

```{r}
```

***
My corpus will be studying the similarities and differences between three Afro-Francophonic artists, Stromae, Maître Gims, and Aya Nakamura. I have always loved listening to Afro-Francophonic music. I am interested in discovering the differences between these artists and if their differing backgrounds plays a role in their songs. In terms of anticipated difference, Stromae is less Afro-pop and more of a techno/pop artist. Maître Gims and Aya Nakamura are rappers but Maître Gims is more of a vocalist. As a result, I want to see if there are bigger differences in tempo, danceabiltiy, and rythm. The core of all three musicians is pop, so I believe there will be a lot of similarities there. I am unsure of how the different musicians' songs will score on Spotify's measurements. Overall, Spotify has alot of the music produced by the artists. I'm unsure if there are gaps in the music selection on Spotify. Some atypical songs from Maitre Gims include: Sapés commes jamais and Hola Señorita. The song is Afro-pop but includes a lot of power vocal moments. Hola Señorita has a lot of Latin Influence and is song in both Spanish and French. Some typical songs from Stromae: Ta fête and Alors on Danse. Both of these songs are incredibly good club/techno/pop music. It is incredibly catchy and has really simple beats. Some typical songs from Aya Nakamura: Djaja and Copines. Both of these songs are incredibly well made Afro-pop/Rap music. However, it is quite a typical song associated to that genre of music.



### Conclusions

```{r}
```

***

I cannot pull any conclusions yet, as I am still in the preliminary processes of this portfolio. 

